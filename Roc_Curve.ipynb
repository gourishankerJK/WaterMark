{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "def get_free_gpu():\n",
    "    gpu_stats = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"])\n",
    "    gpu_stats = gpu_stats.decode('utf-8')\n",
    "    gpu_df = pd.read_csv(io.StringIO(gpu_stats))\n",
    "    gpu_df[\"memory.free\"] = gpu_df[' memory.free [MiB]']\n",
    "    gpu_df['memory.free'] = gpu_df['memory.free'].map(lambda x: x.rstrip(' [MiB]')).astype('float32')\n",
    "    idx = gpu_df['memory.free'].idxmax()\n",
    "    print('Returning GPU{} with {} free MiB'.format(idx, gpu_df.iloc[idx]['memory.free']))\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hey Idiot before going down get the fucking gpu first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This works for LM WaterMarking and Reliability of AI Text detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from LMwatermarking.watermark_processor import  WatermarkDetector\n",
    "\n",
    "\n",
    "def detect(input_text,device=None, tokenizer=None):\n",
    "    watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                        gamma=0.25,\n",
    "                                        seeding_scheme='simple_1',\n",
    "                                        device=device,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        z_threshold=4.0,\n",
    "                                        normalizers=[],\n",
    "                                        select_green_tokens=True)\n",
    "    if len(input_text)-1 > watermark_detector.min_prefix_len:\n",
    "        score_dict = watermark_detector.detect(input_text)\n",
    "        return score_dict\n",
    "\n",
    "\n",
    "def detect_watermark(document):\n",
    "    device = \"cuda:\" + str(get_free_gpu()) if torch.cuda.is_available() else \"cpu\"   \n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"./llama-2-7b-hf\" )\n",
    "    torch.manual_seed(0)\n",
    "    scores = []\n",
    "    for text in document:\n",
    "        score  = detect(text , device , tokenizer)\n",
    "        # print(score)\n",
    "        scores.append(score['z_score'])\n",
    "    return scores\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/Paraphraed_PivotTranslation/sir/llm_watermarked_sir_pivot_translated.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/Paraphraed_PivotTranslation/kwg/llm_watermarked_kwg_pivot_translated.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/Paraphrased_NormalTranslation/sir/llm_watermarked_sir_translated.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/Paraphrased_NormalTranslation/kwg/llm_watermarked_kwg_translated.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/SIR/SIRrephrased4.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/SIR/SIRrephrased0.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/SIR/SIRrephrased2.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/SIR/SIRrephrased1.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/SIR/SIRrephrased3.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/kwg/rephrased2.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/kwg/rephrased0.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/kwg/rephrased4.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/kwg/rephrased3.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Attacked/NewData/translation_paraphrased/kwg/rephrased1.pkl\n",
      "Returning GPU7 with 27941.0 free MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "def zscore_to_percentile(z):\n",
    "    return (1 - np.exp(-0.5 * z**2 / np.pi)) * 100\n",
    "for files in glob.glob('Dataset/Attacked/NewData/*/*/*'):\n",
    "    if('sir' in files.lower() or 'kwg' in files.lower()):\n",
    "        print(files)\n",
    "        with open(files, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        scores = detect_watermark(data)\n",
    "        \n",
    "        vec_zscore_to_percentile = np.vectorize(zscore_to_percentile)\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        scores = vec_zscore_to_percentile(scores)\n",
    "        \n",
    "        \n",
    "        if(os.path.exists(\"/\".join(files.split('/')[:-1]).replace('NewData','NewDataScores'))):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(\"/\".join(files.split('/')[:-1]).replace('NewData','NewDataScores'))\n",
    "        \n",
    "        with open(files.replace('NewData','NewDataScores').replace('.pkl','_scores.pkl'), 'wb') as f:\n",
    "            pickle.dump(scores, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now this is for robust waterMarking scheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning GPU7 with 29265.0 free MiB\n",
      "Hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./RobustWatermark/data/compositional-bert-large-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.819 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tocalculatetruepositives,truenegatives,falsepositives ['Tocalculatetruepositives', ',', 'truenegatives', ',', 'falsepositives']\n",
      "Tocalculatetruepositives,truenegatives,falsepositives,andfalsenegatives,youcanuseaconfusion ['Tocalculatetruepositives', ',', 'truenegatives', ',', 'falsepositives', ',', 'andfalsenegatives', ',', 'youcanuseaconfusion']\n",
      "0.09556977450847626\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import LogitsProcessorList\n",
    "from RobustWatermark.watermark import WatermarkLogitsProcessor, WatermarkWindow, WatermarkContext\n",
    "import argparse\n",
    "import os\n",
    "from transformers import LlamaTokenizer ,  LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "    \n",
    "def detect_watermark(args ):\n",
    "    import subprocess\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import io\n",
    "\n",
    "    def get_free_gpu():\n",
    "        gpu_stats = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"])\n",
    "        gpu_stats = gpu_stats.decode('utf-8')\n",
    "        gpu_df = pd.read_csv(io.StringIO(gpu_stats))\n",
    "        gpu_df[\"memory.free\"] = gpu_df[' memory.free [MiB]']\n",
    "        gpu_df['memory.free'] = gpu_df['memory.free'].map(lambda x: x.rstrip(' [MiB]')).astype('float32')\n",
    "        idx = gpu_df['memory.free'].idxmax()\n",
    "        print('Returning GPU{} with {} free MiB'.format(idx, gpu_df.iloc[idx]['memory.free']))\n",
    "        return idx\n",
    "    device = torch.device(\"cuda:\" + str(get_free_gpu()) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_path = args.llm_path\n",
    "    \n",
    "   # model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "       # model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    if args.watermark_type == \"window\": # use a window of previous tokens to hash, e.g. KGW\n",
    "        watermark_model = WatermarkWindow(device, args.window_size, tokenizer)\n",
    "        logits_processor = WatermarkLogitsProcessor(watermark_model)\n",
    "        \n",
    "        \n",
    "    elif args.watermark_type == \"context\":\n",
    "        print(\"Hello\")\n",
    "        watermark_model = WatermarkContext(device, args.chunk_size, tokenizer, delta = args.delta,transform_model_path=args.transform_model, embedding_model=args.embedding_model)\n",
    "        logits_processor = WatermarkLogitsProcessor(watermark_model)\n",
    "    else:\n",
    "        watermark_model, logits_processor = None, None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_score_generated = watermark_model.detect(\"To calculate true positives, true negatives, false positives, and false negatives, you can use a confusion matrix.\") if watermark_model else 0\n",
    "       # z_score_origin = watermark_model.detect(original_text_file) if watermark_model else 0\n",
    "       # z_score_attacked = watermark_model.detect(attacked_text) if watermark_model else 0\n",
    "        print(z_score_generated)\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "args_dict = {\n",
    "    'watermark_type': 'context',\n",
    "    'base_model': 'llama',\n",
    "    'llm_path': './llama-2-7b-hf',\n",
    "    'window_size': 0,\n",
    "    'generate_number': 200,\n",
    "    'delta': 1.0,\n",
    "    'chunk_size': 10,\n",
    "    'max_new_tokens': 50,\n",
    "    'data_path': 'data/dataset/prompt.pkl',\n",
    "    'output_path': 'SIR.json',\n",
    "    'transform_model': './RobustWatermark/model/transform_model_cbert.pth',\n",
    "    'embedding_model': './RobustWatermark/data/compositional-bert-large-uncased',\n",
    "    'decode_method': 'sample',\n",
    "    'prompt_size': 30,\n",
    "    'beam_size': 5\n",
    "}\n",
    "\n",
    "Args = namedtuple('Args', args_dict.keys())\n",
    "args = Args(*args_dict.values())\n",
    "\n",
    "\n",
    "detect_watermark(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning GPU7 with 27941.0 free MiB\n",
      "Hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./RobustWatermark/data/compositional-bert-large-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tocalculatetruepositives,truenegatives,falsepositives ['Tocalculatetruepositives', ',', 'truenegatives', ',', 'falsepositives']\n",
      "Tocalculatetruepositives,truenegatives,falsepositives,andfalsenegatives,youcanuseaconfusion ['Tocalculatetruepositives', ',', 'truenegatives', ',', 'falsepositives', ',', 'andfalsenegatives', ',', 'youcanuseaconfusion']\n",
      "0.09556977450847626\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "\n",
    "    device = torch.device(\"cuda:\" + str(get_free_gpu()) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_path = args.llm_path\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    if args.watermark_type == \"window\": # use a window of previous tokens to hash, e.g. KGW\n",
    "        watermark_model = WatermarkWindow(device, args.window_size, tokenizer)\n",
    "        logits_processor = WatermarkLogitsProcessor(watermark_model)\n",
    "    elif args.watermark_type == \"context\":\n",
    "        watermark_model = WatermarkContext(device, args.chunk_size, tokenizer, delta = args.delta,transform_model_path=args.transform_model, embedding_model=args.embedding_model)\n",
    "        logits_processor = WatermarkLogitsProcessor(watermark_model)\n",
    "    else:\n",
    "        watermark_model, logits_processor = None, None\n",
    "   \n",
    "        text = \"How are you you\"\n",
    "        words = text.split()\n",
    "        words = words[:args.prompt_size]\n",
    "        begin_text = ' '.join(words)\n",
    "        inputs = tokenizer(begin_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        generation_config = {\n",
    "                \"max_length\": args.max_new_tokens ,\n",
    "                \"no_repeat_ngram_size\": 4,\n",
    "            }\n",
    "        if args.decode_method == \"sample\":\n",
    "            generation_config[\"do_sample\"] = True\n",
    "        elif args.decode_method == \"beam\":\n",
    "            generation_config[\"num_beams\"] = args.beam_size\n",
    "            generation_config[\"do_sample\"] = False\n",
    "        \n",
    "        if watermark_model is not None:\n",
    "            generation_config[\"logits_processor\"] = LogitsProcessorList([logits_processor])\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **generation_config)\n",
    "            print(outputs)\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "args_dict = {\n",
    "    'watermark_type': 'context',\n",
    "    'base_model': 'llama',\n",
    "    'llm_path': './llama-2-7b-hf',\n",
    "    'window_size': 0,\n",
    "    'generate_number': 200,\n",
    "    'delta': 1.0,\n",
    "    'chunk_size': 10,\n",
    "    'max_new_tokens': 50,\n",
    "    'data_path': 'data/dataset/prompt.pkl',\n",
    "    'output_path': 'SIR.json',\n",
    "    'transform_model': './RobustWatermark/model/transform_model_cbert.pth',\n",
    "    'embedding_model': './RobustWatermark/data/compositional-bert-large-uncased',\n",
    "    'decode_method': 'sample',\n",
    "    'prompt_size': 30,\n",
    "    'beam_size': 5\n",
    "}\n",
    "\n",
    "Args = namedtuple('Args', args_dict.keys())\n",
    "args = Args(*args_dict.values())\n",
    "\n",
    "\n",
    "detect_watermark(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
